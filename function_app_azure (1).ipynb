{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60937d29-ffad-4084-a9bf-7f33aa76f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import azure.functions as func\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "APP_ID = \"92c0a55a\"\n",
    "APP_KEY = \"d58f307a1667e2e502b8da4ad83ef81c\"\n",
    "\n",
    "COUNTRY = \"au\"\n",
    "QUERIES = [\"data scientist\", \"software engineer\", \"cybersecurity\", \"ai engineer\"]\n",
    "PAGES_PER_ROLE = 2\n",
    "\n",
    "SQL_SERVER = \"auseast.database.windows.net\"\n",
    "SQL_PORT = 1433\n",
    "SQL_DATABASE = \"JobMarketDb\"\n",
    "SQL_USERNAME = \"CloudSAfc8188ca\"\n",
    "#Database password hidden\n",
    "SQL_PASSWORD = \"passwordishiddem\"\n",
    "\n",
    "CONN_STR = (\n",
    "    f\"mssql+pytds://{SQL_USERNAME}:{SQL_PASSWORD}\"\n",
    "    f\"@{SQL_SERVER}:{SQL_PORT}/{SQL_DATABASE}\"\n",
    "    \"?encrypt=yes&trustservercertificate=no&hostNameInCertificate=*.database.windows.net\"\n",
    ")\n",
    "\n",
    "app = func.FunctionApp()\n",
    "\n",
    "@app.timer_trigger(\n",
    "    schedule=\"0 0 0 * * *\", \n",
    "    arg_name=\"myTimer\",\n",
    "    run_on_startup=True\n",
    ")\n",
    "def timer_trigger(myTimer: func.TimerRequest):\n",
    "\n",
    "    logging.info(\"ðŸ’¼ Job ingestion function started.\")\n",
    "\n",
    "    #Fetching job data\n",
    "    all_rows = []\n",
    "    for role in QUERIES:\n",
    "        for page in range(1, PAGES_PER_ROLE + 1):\n",
    "\n",
    "            url = f\"https://api.adzuna.com/v1/api/jobs/{COUNTRY}/search/{page}\"\n",
    "            params = {\n",
    "                \"app_id\": APP_ID,\n",
    "                \"app_key\": APP_KEY,\n",
    "                \"what\": role,\n",
    "                \"results_per_page\": 50,\n",
    "                \"content-type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "            res = requests.get(url, params=params)\n",
    "            data = res.json()\n",
    "\n",
    "            for job in data.get(\"results\", []):\n",
    "                all_rows.append({\n",
    "                    \"role\": role,\n",
    "                    \"title\": job[\"title\"],\n",
    "                    \"company\": job.get(\"company\", {}).get(\"display_name\"),\n",
    "                    \"location\": job.get(\"location\", {}).get(\"display_name\"),\n",
    "                    \"created\": job[\"created\"],\n",
    "                    \"salary_min\": job.get(\"salary_min\"),\n",
    "                    \"salary_max\": job.get(\"salary_max\"),\n",
    "                    \"description\": job.get(\"description\", \"\")[:500],\n",
    "                    \"fetched_at\": datetime.utcnow()\n",
    "                })\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    logging.info(f\"Downloaded {len(df)} job listings\")\n",
    "\n",
    "    df[\"created\"] = pd.to_datetime(df[\"created\"], utc=True)\n",
    "    df[\"salary_min\"] = df.groupby(\"role\")[\"salary_min\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df[\"salary_max\"] = df.groupby(\"role\")[\"salary_max\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df[\"salary_avg\"] = df[[\"salary_min\", \"salary_max\"]].mean(axis=1)\n",
    "\n",
    "    logging.info(\"Data cleaning complete\")\n",
    "\n",
    "    try:\n",
    "        engine = create_engine(CONN_STR, connect_args={\"autocommit\": True})\n",
    "\n",
    "        insert_sql = text(\"\"\"\n",
    "            INSERT INTO JobPosts (\n",
    "                role, title, company, location, created,\n",
    "                salary_min, salary_max, salary_avg, description, fetched_at\n",
    "            )\n",
    "            VALUES (\n",
    "                :role, :title, :company, :location, :created,\n",
    "                :salary_min, :salary_max, :salary_avg, :description, :fetched_at\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        allowed_cols = [\n",
    "            \"role\", \"title\", \"company\", \"location\", \"created\",\n",
    "            \"salary_min\", \"salary_max\", \"salary_avg\", \"description\", \"fetched_at\"\n",
    "        ]\n",
    "\n",
    "        rows = df[allowed_cols].to_dict(orient=\"records\")\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            for row in rows:\n",
    "                conn.execute(insert_sql, row)\n",
    "\n",
    "        logging.info(\"âœ… Uploaded all jobs into Azure SQL successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SQL Insert failed: {e}\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Job ingestion pipeline finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4184ca5-d4a5-4819-a79d-b37e0d319a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
